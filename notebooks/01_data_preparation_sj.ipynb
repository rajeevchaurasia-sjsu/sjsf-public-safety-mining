{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf38813f",
   "metadata": {},
   "source": [
    "# Data Preparation: San Jose SJPD Incidents\n",
    "\n",
    "** Project:** A Tale of Two Cities - Comparative Public Safety Analysis\n",
    "\n",
    "**Purpose:** This notebook handles data loading, cleaning, preprocessing, and feature engineering for the San Jose dataset.\n",
    "\n",
    "**Output:** Clean, analysis-ready dataset saved to `data/processed/sj_calls_cleaned.csv`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1dfb81",
   "metadata": {},
   "source": [
    "# 1. Import Libraries and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5dec1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b0b8e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 CSV files:\n",
      "  - policecalls2018.csv\n",
      "  - policecalls2019.csv\n",
      "  - policecalls2020.csv\n",
      "  - policecalls2021.csv\n",
      "  - policecalls2022.csv\n",
      "  - policecalls2023.csv\n",
      "  - policecalls2024.csv\n",
      "  - policecalls2025.csv\n",
      "Loading: policecalls2018.csv...\n",
      "  Rows: 322,365\n",
      "Loading: policecalls2019.csv...\n",
      "  Rows: 322,624\n",
      "Loading: policecalls2020.csv...\n",
      "  Rows: 297,458\n",
      "Loading: policecalls2021.csv...\n",
      "  Rows: 302,119\n",
      "Loading: policecalls2022.csv...\n",
      "  Rows: 294,515\n",
      "Loading: policecalls2023.csv...\n",
      "  Rows: 286,481\n",
      "Loading: policecalls2024.csv...\n",
      "  Rows: 276,069\n",
      "Loading: policecalls2025.csv...\n",
      "  Rows: 225,561\n",
      "\n",
      "Concatenating all files...\n",
      "\n",
      "✅ Dataset loaded: 2,327,192 rows, 15 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CDTS</th>\n",
       "      <th>EID</th>\n",
       "      <th>START_DATE</th>\n",
       "      <th>CALL_NUMBER</th>\n",
       "      <th>PRIORITY</th>\n",
       "      <th>REPORT_DATE</th>\n",
       "      <th>OFFENSE_DATE</th>\n",
       "      <th>OFFENSE_TIME</th>\n",
       "      <th>CALLTYPE_CODE</th>\n",
       "      <th>CALL_TYPE</th>\n",
       "      <th>FINAL_DISPO_CODE</th>\n",
       "      <th>FINAL_DISPO</th>\n",
       "      <th>ADDRESS</th>\n",
       "      <th>CITY</th>\n",
       "      <th>STATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20180101000426PS</td>\n",
       "      <td>7000038</td>\n",
       "      <td>5/14/2021 12:00:00 AM</td>\n",
       "      <td>P180010001</td>\n",
       "      <td>2</td>\n",
       "      <td>1/1/2018 12:00:00 AM</td>\n",
       "      <td>1/1/2018 12:00:00 AM</td>\n",
       "      <td>00:00:02</td>\n",
       "      <td>FDAID</td>\n",
       "      <td>FIRE DEPARTMENT REQUEST FOR PD</td>\n",
       "      <td>N</td>\n",
       "      <td>No report required; dispatch r</td>\n",
       "      <td>[2900]-[3000] ALUM ROCK AV</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20180101000120PS</td>\n",
       "      <td>7000040</td>\n",
       "      <td>5/14/2021 12:00:00 AM</td>\n",
       "      <td>P180010003</td>\n",
       "      <td>3</td>\n",
       "      <td>1/1/2018 12:00:00 AM</td>\n",
       "      <td>1/1/2018 12:00:00 AM</td>\n",
       "      <td>00:00:15</td>\n",
       "      <td>1033A</td>\n",
       "      <td>ALARM, AUDIBLE</td>\n",
       "      <td>NR</td>\n",
       "      <td>No Response</td>\n",
       "      <td>[4700]-[4800] CALENDULA CT</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20180101003329PS</td>\n",
       "      <td>7000041</td>\n",
       "      <td>5/14/2021 12:00:00 AM</td>\n",
       "      <td>P180010004</td>\n",
       "      <td>4</td>\n",
       "      <td>1/1/2018 12:00:00 AM</td>\n",
       "      <td>1/1/2018 12:00:00 AM</td>\n",
       "      <td>00:00:32</td>\n",
       "      <td>415M</td>\n",
       "      <td>DISTURBANCE, MUSIC</td>\n",
       "      <td>CAN</td>\n",
       "      <td>Canceled</td>\n",
       "      <td>[3100]-[3200] WILLIAMSBURG DR</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20180101000358PS</td>\n",
       "      <td>7000043</td>\n",
       "      <td>5/14/2021 12:00:00 AM</td>\n",
       "      <td>P180010005</td>\n",
       "      <td>4</td>\n",
       "      <td>1/1/2018 12:00:00 AM</td>\n",
       "      <td>1/1/2018 12:00:00 AM</td>\n",
       "      <td>00:01:41</td>\n",
       "      <td>415FC</td>\n",
       "      <td>DISTURBANCE, FIRECRACKERS</td>\n",
       "      <td>CAN</td>\n",
       "      <td>Canceled</td>\n",
       "      <td>[1300]-[1400] BOURET DR</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20180101000535PS</td>\n",
       "      <td>7000044</td>\n",
       "      <td>5/14/2021 12:00:00 AM</td>\n",
       "      <td>P180010006</td>\n",
       "      <td>4</td>\n",
       "      <td>1/1/2018 12:00:00 AM</td>\n",
       "      <td>1/1/2018 12:00:00 AM</td>\n",
       "      <td>00:01:53</td>\n",
       "      <td>415M</td>\n",
       "      <td>DISTURBANCE, MUSIC</td>\n",
       "      <td>CAN</td>\n",
       "      <td>Canceled</td>\n",
       "      <td>[400]-[500] WASHINGTON ST</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               CDTS      EID             START_DATE CALL_NUMBER  PRIORITY  \\\n",
       "0  20180101000426PS  7000038  5/14/2021 12:00:00 AM  P180010001         2   \n",
       "1  20180101000120PS  7000040  5/14/2021 12:00:00 AM  P180010003         3   \n",
       "2  20180101003329PS  7000041  5/14/2021 12:00:00 AM  P180010004         4   \n",
       "3  20180101000358PS  7000043  5/14/2021 12:00:00 AM  P180010005         4   \n",
       "4  20180101000535PS  7000044  5/14/2021 12:00:00 AM  P180010006         4   \n",
       "\n",
       "            REPORT_DATE          OFFENSE_DATE OFFENSE_TIME CALLTYPE_CODE  \\\n",
       "0  1/1/2018 12:00:00 AM  1/1/2018 12:00:00 AM     00:00:02         FDAID   \n",
       "1  1/1/2018 12:00:00 AM  1/1/2018 12:00:00 AM     00:00:15         1033A   \n",
       "2  1/1/2018 12:00:00 AM  1/1/2018 12:00:00 AM     00:00:32          415M   \n",
       "3  1/1/2018 12:00:00 AM  1/1/2018 12:00:00 AM     00:01:41         415FC   \n",
       "4  1/1/2018 12:00:00 AM  1/1/2018 12:00:00 AM     00:01:53          415M   \n",
       "\n",
       "                        CALL_TYPE FINAL_DISPO_CODE  \\\n",
       "0  FIRE DEPARTMENT REQUEST FOR PD                N   \n",
       "1                  ALARM, AUDIBLE               NR   \n",
       "2              DISTURBANCE, MUSIC              CAN   \n",
       "3       DISTURBANCE, FIRECRACKERS              CAN   \n",
       "4              DISTURBANCE, MUSIC              CAN   \n",
       "\n",
       "                      FINAL_DISPO                        ADDRESS      CITY  \\\n",
       "0  No report required; dispatch r     [2900]-[3000] ALUM ROCK AV  San Jose   \n",
       "1                     No Response     [4700]-[4800] CALENDULA CT  San Jose   \n",
       "2                        Canceled  [3100]-[3200] WILLIAMSBURG DR  San Jose   \n",
       "3                        Canceled        [1300]-[1400] BOURET DR  San Jose   \n",
       "4                        Canceled      [400]-[500] WASHINGTON ST  San Jose   \n",
       "\n",
       "  STATE  \n",
       "0    CA  \n",
       "1    CA  \n",
       "2    CA  \n",
       "3    CA  \n",
       "4    CA  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the directory containing the San Jose CSV files\n",
    "sj_data_path = '../data/raw/sj_calls_for_service/'\n",
    "\n",
    "# Get a list of all the CSV file names in the directory\n",
    "csv_files = [f for f in os.listdir(sj_data_path) if f.endswith('.csv')]\n",
    "csv_files.sort()  # Sort files for consistent loading order\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for file in csv_files:\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "# Create an empty list to hold the individual DataFrames\n",
    "df_list = []\n",
    "\n",
    "# Loop through each CSV file, load it, and add it to the list\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(sj_data_path, file)\n",
    "    print(f\"Loading: {file}...\")\n",
    "    temp_df = pd.read_csv(file_path)\n",
    "    print(f\"  Rows: {len(temp_df):,}\")\n",
    "    df_list.append(temp_df)\n",
    "\n",
    "# Concatenate all the DataFrames into a single master DataFrame\n",
    "print(\"\\nConcatenating all files...\")\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "print(f\"\\n✅ Dataset loaded: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b9e0cd",
   "metadata": {},
   "source": [
    "# 2. Initial Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e371c104",
   "metadata": {},
   "source": [
    "## 2.1 Checking for Duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "535ad7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for duplicate IDs...\n",
      "Duplicate EIDs: 45,812\n",
      "Dropping duplicate rows based on 'EID'...\n",
      "New row count: 2,281,380\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking for duplicate IDs...\")\n",
    "eid_duplicates = df.duplicated(subset=['EID']).sum()\n",
    "\n",
    "print(f\"Duplicate EIDs: {eid_duplicates:,}\")\n",
    "\n",
    "if eid_duplicates > 0:\n",
    "    print(\"Dropping duplicate rows based on 'EID'...\")\n",
    "    df.drop_duplicates(subset=['EID'], inplace=True)\n",
    "    print(f\"New row count: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8771f1",
   "metadata": {},
   "source": [
    "## 2.2 Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d298332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values (%):\n",
      "ADDRESS      2.899298\n",
      "CALL_TYPE    0.001271\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate missing value percentages\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "print(\"Columns with missing values (%):\")\n",
    "print(missing_percentage[missing_percentage > 0].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3edfd0",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "### 3.1 DateTime Processing\n",
    "Combine OFFENSE_DATE and OFFENSE_TIME into a single Incident DateTime index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a80c8c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m1/ts8y18jx50l9f_z5zmbcrypr0000gn/T/ipykernel_7807/2398794308.py:1: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  date_str = pd.to_datetime(df['OFFENSE_DATE'], errors='coerce').dt.strftime('%Y-%m-%d')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DateTime Parsing Complete.\n",
      "Total Rows: 2,281,380\n",
      "Successful: 2,281,380\n",
      "Errors (NaT): 0\n",
      "\n",
      "Sample of successfully parsed dates:\n",
      "           OFFENSE_DATE OFFENSE_TIME   Incident DateTime\n",
      "0  1/1/2018 12:00:00 AM     00:00:02 2018-01-01 00:00:02\n",
      "1  1/1/2018 12:00:00 AM     00:00:15 2018-01-01 00:00:15\n",
      "2  1/1/2018 12:00:00 AM     00:00:32 2018-01-01 00:00:32\n",
      "3  1/1/2018 12:00:00 AM     00:01:41 2018-01-01 00:01:41\n",
      "4  1/1/2018 12:00:00 AM     00:01:53 2018-01-01 00:01:53\n"
     ]
    }
   ],
   "source": [
    "date_str = pd.to_datetime(df['OFFENSE_DATE'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "time_str = pd.to_datetime(df['OFFENSE_TIME'], format='%H:%M:%S', errors='coerce').dt.strftime('%H:%M:%S')\n",
    "\n",
    "# Combine OFFENSE_DATE and OFFENSE_TIME into a single datetime column\n",
    "full_datetime_str = date_str + ' ' + time_str\n",
    "df['Incident DateTime'] = pd.to_datetime(full_datetime_str, errors='coerce')\n",
    "\n",
    "# Count and report any parsing errors\n",
    "errors = df['Incident DateTime'].isna().sum()\n",
    "total = len(df)\n",
    "print(f\"DateTime Parsing Complete.\")\n",
    "print(f\"Total Rows: {total:,}\")\n",
    "print(f\"Successful: {total - errors:,}\")\n",
    "print(f\"Errors (NaT): {errors:,}\")\n",
    "\n",
    "# Show successful examples\n",
    "print(\"\\nSample of successfully parsed dates:\")\n",
    "print(df.head()[['OFFENSE_DATE', 'OFFENSE_TIME', 'Incident DateTime']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "111c9d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Index set and redundant columns dropped\n",
      "Columns remaining: 7\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 2281380 entries, 2018-01-01 00:00:02 to 2025-11-08 23:56:34\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Dtype \n",
      "---  ------            ----- \n",
      " 0   PRIORITY          int64 \n",
      " 1   CALLTYPE_CODE     object\n",
      " 2   CALL_TYPE         object\n",
      " 3   FINAL_DISPO_CODE  object\n",
      " 4   FINAL_DISPO       object\n",
      " 5   ADDRESS           object\n",
      " 6   CITY              object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 139.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Set the new 'Incident DateTime' column as the DataFrame's index\n",
    "df.set_index('Incident DateTime', inplace=True)\n",
    "\n",
    "# Drop the original date and time columns as they are now redundant\n",
    "date_time_cols = ['START_DATE', 'REPORT_DATE', 'OFFENSE_DATE', 'OFFENSE_TIME']\n",
    "df.drop(date_time_cols, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "# Drop columns not relevant for comparative analysis\n",
    "# EID, CDTS, CALL_NUMBER are internal identifiers\n",
    "# STATE is redundant (all data is from California)\n",
    "identifier_cols = ['EID', 'CDTS', 'CALL_NUMBER', 'STATE']\n",
    "df.drop(identifier_cols, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "print(f\"✅ Index set and redundant columns dropped\")\n",
    "print(f\"Columns remaining: {len(df.columns)}\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2279e3",
   "metadata": {},
   "source": [
    "## 3.2 Handling Missing Values\n",
    "\n",
    "Now we drop all rows that had NaT dates or any other missing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "576ce17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows dropped: 66,173 (2.90%)\n",
      "Clean dataset: 2,215,207 rows\n",
      "Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with remaining missing values\n",
    "rows_before = len(df)\n",
    "df.dropna(inplace=True)\n",
    "rows_after = len(df)\n",
    "\n",
    "print(f\"Rows dropped: {rows_before - rows_after:,} ({((rows_before - rows_after)/rows_before)*100:.2f}%)\")\n",
    "print(f\"Clean dataset: {rows_after:,} rows\")\n",
    "print(f\"Remaining missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2618b222",
   "metadata": {},
   "source": [
    "## 3.3 Handling Duplicate Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f37e36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 1,046,383 full-row duplicates.\n"
     ]
    }
   ],
   "source": [
    "# Handling remaining duplicates\n",
    "\n",
    "rows_before_final_drop = len(df)\n",
    "df.drop_duplicates(inplace=True)\n",
    "rows_after_final_drop = len(df)\n",
    "\n",
    "print(f\"Dropped {rows_before_final_drop - rows_after_final_drop:,} full-row duplicates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7652b7b8",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8153fa36",
   "metadata": {},
   "source": [
    "## 4.1 Temporal Features\n",
    "\n",
    "Extract temporal features for time-series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "314e3363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Temporal features created:\n",
      "                     Hour Day of Week Name Month Name  Year  Is Weekend\n",
      "Incident DateTime                                                      \n",
      "2018-01-01 00:00:02     0           Monday    January  2018           0\n",
      "2018-01-01 00:00:15     0           Monday    January  2018           0\n",
      "2018-01-01 00:00:32     0           Monday    January  2018           0\n",
      "2018-01-01 00:01:41     0           Monday    January  2018           0\n",
      "2018-01-01 00:01:53     0           Monday    January  2018           0\n"
     ]
    }
   ],
   "source": [
    "# Create temporal features from the DateTime index\n",
    "df['Hour'] = df.index.hour\n",
    "df['Day'] = df.index.day\n",
    "df['Month'] = df.index.month\n",
    "df['Year'] = df.index.year\n",
    "df['Day of Week'] = df.index.dayofweek  # Monday=0, Sunday=6\n",
    "df['Day of Week Name'] = df.index.day_name()\n",
    "df['Month Name'] = df.index.month_name()\n",
    "df['Quarter'] = df.index.quarter\n",
    "df['Is Weekend'] = df['Day of Week'].isin([5, 6]).astype(int)\n",
    "\n",
    "print(\"✅ Temporal features created:\")\n",
    "print(df[['Hour', 'Day of Week Name', 'Month Name', 'Year', 'Is Weekend']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b39c237",
   "metadata": {},
   "source": [
    "## 4.2 Location Features\n",
    "\n",
    "Extract a clean street name from the ADDRESS field for grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29df088d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'Clean_Address' feature. Examples:\n",
      "                                           ADDRESS    Clean_Address\n",
      "Incident DateTime                                                  \n",
      "2018-01-01 00:00:02     [2900]-[3000] ALUM ROCK AV     ALUM ROCK AV\n",
      "2018-01-01 00:00:15     [4700]-[4800] CALENDULA CT     CALENDULA CT\n",
      "2018-01-01 00:00:32  [3100]-[3200] WILLIAMSBURG DR  WILLIAMSBURG DR\n",
      "2018-01-01 00:01:41        [1300]-[1400] BOURET DR        BOURET DR\n",
      "2018-01-01 00:01:53      [400]-[500] WASHINGTON ST    WASHINGTON ST\n"
     ]
    }
   ],
   "source": [
    "# Clean the ADDRESS field by removing patterns like '[number]-[number] '\n",
    "df['Clean_Address'] = df['ADDRESS'].str.replace(r'\\[\\d+\\]-\\[\\d+\\]\\s', '', regex=True)\n",
    "\n",
    "print(\"Created 'Clean_Address' feature. Examples:\")\n",
    "print(df[['ADDRESS', 'Clean_Address']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aca25fb",
   "metadata": {},
   "source": [
    "## 4.3 Category Harmonization\n",
    "\n",
    "We need to map the granular CALL_TYPE to high-level categories that can be compared with San Francisco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51d76a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original unique SJ Categories: 241\n",
      "\n",
      "High-level categories created:\n",
      "Incident_High_Level_Category\n",
      "Disturbance/Suspicious    439807\n",
      "Traffic/Vehicle           238636\n",
      "Other                     192200\n",
      "Theft/Property            127662\n",
      "Alarm                      62575\n",
      "Violent                    55084\n",
      "Non-Criminal/Admin         52860\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nOriginal unique SJ Categories: {df['CALL_TYPE'].nunique()}\")\n",
    "\n",
    "def map_high_level_category(cat):\n",
    "    cat = str(cat).upper()\n",
    "\n",
    "    # Violent\n",
    "    if any(s in cat for s in ['ASSAULT', 'ROBBERY', 'HOMICIDE', 'RAPE', 'BATTERY', 'WEAPON']):\n",
    "        return 'Violent'\n",
    "    \n",
    "    # Theft/Property\n",
    "    if any(s in cat for s in ['BURGLARY', 'THEFT', 'STOLEN VEHICLE', 'MALICIOUS MISCHIEF', 'VANDALISM']):\n",
    "        return 'Theft/Property'\n",
    "    \n",
    "    # Disturbance/Suspicious\n",
    "    if any(s in cat for s in ['DISTURBANCE', 'SUSPICIOUS', 'WELFARE CHECK', 'TRESPASSING']):\n",
    "        return 'Disturbance/Suspicious'\n",
    "\n",
    "    # Traffic/Vehicle\n",
    "    if any(s in cat for s in ['TRAFFIC', 'PARKING VIOLATION', 'VEHICLE STOP', 'ACCIDENT', 'RECKLESS DRIVING', 'HIT AND RUN', 'PEDESTRIAN STOP']):\n",
    "        return 'Traffic/Vehicle'\n",
    "\n",
    "    # Alarm\n",
    "    if 'ALARM' in cat:\n",
    "        return 'Alarm'\n",
    "    \n",
    "    # Non-Criminal / Admin\n",
    "    if any(s in cat for s in ['MEET THE CITIZEN', 'UNK TYPE 911 CALL', 'RECOVERED STOLEN VEHICLE']):\n",
    "        return 'Non-Criminal/Admin'\n",
    "    \n",
    "    # All others fall into a general 'Other'\n",
    "    return 'Other'\n",
    "\n",
    "# Apply the mapping\n",
    "df['Incident_High_Level_Category'] = df['CALL_TYPE'].apply(map_high_level_category)\n",
    "\n",
    "print(\"\\nHigh-level categories created:\")\n",
    "print(df['Incident_High_Level_Category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aadcab",
   "metadata": {},
   "source": [
    "# 5. Final Schema Synchronization\n",
    "\n",
    "To allow for direct comparison, we rename the key SJ columns to match the names and intent of the SF dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28a728c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harmonizing column names for comparison...\n",
      "SJ columns renamed.\n"
     ]
    }
   ],
   "source": [
    "print(\"Harmonizing column names for comparison...\")\n",
    "\n",
    "rename_map = {\n",
    "    'CALL_TYPE': 'Incident_Category',\n",
    "    'FINAL_DISPO': 'Resolution',\n",
    "    'Clean_Address': 'Address_Clean',\n",
    "    'CITY': 'City'\n",
    "}\n",
    "\n",
    "# Only rename columns that exist\n",
    "cols_to_rename = {k: v for k, v in rename_map.items() if k in df.columns}\n",
    "df.rename(columns=cols_to_rename, inplace=True)\n",
    "\n",
    "print(f\"SJ columns renamed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb5e1c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting final schema synchronization...\n",
      "Columns before cleanup: 18\n",
      "Columns after cleanup: 13\n",
      "Final column list:\n",
      "['Incident_High_Level_Category', 'Resolution', 'Neighborhood', 'Police_District', 'Hour', 'Day', 'Month', 'Year', 'Day_of_Week', 'Day_of_Week_Name', 'Month_Name', 'Quarter', 'Is_Weekend']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting final schema synchronization...\")\n",
    "print(f\"Columns before cleanup: {len(df.columns)}\")\n",
    "\n",
    "# Define final columns to keep\n",
    "columns_to_keep = {\n",
    "    # Key Fields\n",
    "    'Incident_High_Level_Category': 'Incident_High_Level_Category',\n",
    "    'Resolution': 'Resolution',\n",
    "    'Address_Clean': 'Neighborhood',\n",
    "    'City': 'Police_District',   \n",
    "    \n",
    "    # Temporal Features\n",
    "    'Hour': 'Hour',\n",
    "    'Day': 'Day',\n",
    "    'Month': 'Month',\n",
    "    'Year': 'Year',\n",
    "    'Day of Week': 'Day_of_Week',\n",
    "    'Day of Week Name': 'Day_of_Week_Name',\n",
    "    'Month Name': 'Month_Name',\n",
    "    'Quarter': 'Quarter',\n",
    "    'Is Weekend': 'Is_Weekend'\n",
    "}\n",
    "\n",
    "# Filter DataFrame\n",
    "df_final = df[columns_to_keep.keys()].copy()\n",
    "\n",
    "# Rename columns for perfect sync\n",
    "df_final.rename(columns=columns_to_keep, inplace=True)\n",
    "\n",
    "print(f\"Columns after cleanup: {len(df_final.columns)}\")\n",
    "print(\"Final column list:\")\n",
    "print(df_final.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c7b38a",
   "metadata": {},
   "source": [
    "## 6. Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e5324b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL CLEAN DATASET SUMMARY\n",
      "============================================================\n",
      "Total Rows: 1,168,824\n",
      "Total Columns: 13\n",
      "Date Range: 2018-01-01 00:00:02 to 2025-11-08 23:55:37\n",
      "Missing Values: 0\n",
      "Duplicate Rows: 2214\n",
      "\n",
      "Column List:\n",
      "['Incident_High_Level_Category', 'Resolution', 'Neighborhood', 'Police_District', 'Hour', 'Day', 'Month', 'Year', 'Day_of_Week', 'Day_of_Week_Name', 'Month_Name', 'Quarter', 'Is_Weekend']\n"
     ]
    }
   ],
   "source": [
    "# Final data quality summary\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL CLEAN DATASET SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Rows: {len(df_final):,}\")\n",
    "print(f\"Total Columns: {len(df_final.columns)}\")\n",
    "print(f\"Date Range: {df_final.index.min()} to {df_final.index.max()}\")\n",
    "print(f\"Missing Values: {df_final.isnull().sum().sum()}\") # Will show values for Lat/Lon, this is OK\n",
    "print(f\"Duplicate Rows: {df_final.duplicated().sum()}\")\n",
    "print(\"\\nColumn List:\")\n",
    "print(df_final.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1624514d",
   "metadata": {},
   "source": [
    "## 6. Save Processed Data\n",
    "\n",
    "Save the clean dataset for use by team members in downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aff93b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Clean dataset saved to: ../data/processed/sj_calls_cleaned.csv\n",
      "File size: 133.80 MB\n"
     ]
    }
   ],
   "source": [
    "# Create processed data directory if it doesn't exist\n",
    "output_dir = Path('../data/processed')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = output_dir / 'sj_calls_cleaned.csv'\n",
    "df_final.to_csv(output_path)\n",
    "\n",
    "print(f\"✅ Clean dataset saved to: {output_path}\")\n",
    "print(f\"File size: {output_path.stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f12cdb9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Note\n",
    "\n",
    "**For Team Members:**\n",
    "- Load this clean dataset using: `pd.read_csv('../data/processed/sj_calls_cleaned.csv', index_col='Incident DateTime', parse_dates=True)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

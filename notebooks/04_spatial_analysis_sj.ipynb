{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a08d80",
   "metadata": {},
   "source": [
    "# Spatial Analysis: San Jose (DBSCAN)\n",
    "\n",
    "**Project:** A Tale of Two Cities - Comparative Public Safety Analysis\n",
    "\n",
    "**Purpose:** This notebook handles the spatial analysis for San Jose. It uses DBSCAN clustering to identify public safety hotspots from the processed data.\n",
    "\n",
    "**Input:** `data/processed/sj_calls_cleaned.csv`\n",
    "\n",
    "**Analysis:**\n",
    "1.  Load the cleaned data.\n",
    "2.  Sample the data for efficient clustering.\n",
    "3.  Determine optimal DBSCAN parameters (`eps`).\n",
    "4.  Run DBSCAN to find clusters.\n",
    "5.  Visualize the resulting clusters on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c82500dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello, World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be48317",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2483912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 10)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3644bb73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "411633b9",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f24e0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 1,168,824 rows\n",
      "Date range: 2018-01-01 00:00:02 to 2025-11-08 23:55:37\n",
      "\n",
      "Columns: ['Incident_High_Level_Category', 'Resolution', 'Neighborhood', 'Police_District', 'Hour', 'Day', 'Month', 'Year', 'Day_of_Week', 'Day_of_Week_Name', 'Month_Name', 'Quarter', 'Is_Weekend']\n",
      "\n",
      "\n",
      "CRITICAL ERROR: 'Latitude' or 'Longitude' columns not found in the data.\n",
      "Please run the updated '01_data_preparation_sj.ipynb' with the geocoding step first.\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned SJ calls data\n",
    "# Note: This file was created by the '01_data_preparation_sj.ipynb' notebook\n",
    "df = pd.read_csv(\n",
    "    '../data/processed/sj_calls_cleaned.csv',\n",
    "    index_col='Incident DateTime',\n",
    "    parse_dates=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded: {len(df):,} rows\")\n",
    "print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "\n",
    "# Display basic statistics\n",
    "# We must check if Latitude and Longitude columns exist\n",
    "if 'Latitude' in df.columns and 'Longitude' in df.columns:\n",
    "    print(\"\\nSpatial Data Summary:\")\n",
    "    print(df[['Latitude', 'Longitude']].describe())\n",
    "else:\n",
    "    print(\"\\n\\nCRITICAL ERROR: 'Latitude' or 'Longitude' columns not found in the data.\")\n",
    "    print(\"Please run the updated '01_data_preparation_sj.ipynb' with the geocoding step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e848aa",
   "metadata": {},
   "source": [
    "## 3. Exploratory Spatial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db24ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will fail if Lat/Lon are missing\n",
    "if 'Latitude' in df.columns and 'Longitude' in df.columns:\n",
    "    print(f\"Total incidents: {len(df):,}\")\n",
    "    print(f\"Latitude range: [{df['Latitude'].min():.6f}, {df['Latitude'].max():.6f}]\")\n",
    "    print(f\"Longitude range: [{df['Longitude'].min():.6f}, {df['Longitude'].max():.6f}]\")\n",
    "\n",
    "    # Create hexbin density plot (better for 1M+ points)\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.hexbin(df['Longitude'], df['Latitude'], gridsize=50, cmap='YlOrRd', mincnt=1)\n",
    "    plt.colorbar(label='Incident Count')\n",
    "    plt.xlabel('Longitude', fontsize=12)\n",
    "    plt.ylabel('Latitude', fontsize=12)\n",
    "    plt.title('San Jose Incident Density Heatmap', fontsize=16, weight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot perform Exploratory Spatial Analysis: Missing coordinate data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8000fb6b",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9151b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will fail if Lat/Lon are missing\n",
    "if 'Latitude' in df.columns and 'Longitude' in df.columns:\n",
    "    # Extract coordinates for clustering\n",
    "    coords = df[['Latitude', 'Longitude']].values\n",
    "\n",
    "    print(f\"Total incidents for clustering: {len(coords):,}\")\n",
    "    print(f\"Coordinates shape: {coords.shape}\")\n",
    "\n",
    "    # For computational efficiency, we'll sample 100,000 points\n",
    "    MAX_SAMPLES = 100000\n",
    "\n",
    "    print(f\"\\nDataset is large ({len(coords):,} points). Sampling {MAX_SAMPLES:,} points for analysis...\")\n",
    "    np.random.seed(42) # Use same seed as Tushar for reproducibility\n",
    "    sample_indices = np.random.choice(len(coords), size=MAX_SAMPLES, replace=False)\n",
    "    \n",
    "    coords_sample = coords[sample_indices]\n",
    "    df_sample = df.iloc[sample_indices].copy()\n",
    "    \n",
    "    print(f\"Sample created: {len(coords_sample):,} points\")\n",
    "else:\n",
    "    print(\"Cannot prepare data for DBSCAN: Missing coordinate data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b677ca93",
   "metadata": {},
   "source": [
    "## 5. Determine Optimal DBSCAN Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b13a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will fail if Lat/Lon are missing\n",
    "if 'coords_sample' in locals():\n",
    "    # Use k-nearest neighbors to determine optimal eps\n",
    "    min_samples = 4 # Same as Tushar\n",
    "    \n",
    "    print(f\"Computing k-nearest neighbors for {len(coords_sample):,} points...\")\n",
    "    neighbors = NearestNeighbors(n_neighbors=min_samples)\n",
    "    neighbors_fit = neighbors.fit(coords_sample)\n",
    "    distances, indices = neighbors_fit.kneighbors(coords_sample)\n",
    "\n",
    "    # Sort distances\n",
    "    distances = np.sort(distances[:, min_samples-1], axis=0)\n",
    "\n",
    "    # Plot k-distance graph\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(distances)\n",
    "    plt.xlabel('Points sorted by distance', fontsize=12)\n",
    "    plt.ylabel(f'{min_samples}-th Nearest Neighbor Distance', fontsize=12)\n",
    "    plt.title('K-Distance Graph for Optimal Eps Selection', fontsize=16, weight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add a zoomed-in plot, as the 'elbow' will be very low\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(distances)\n",
    "    plt.xlabel('Points sorted by distance', fontsize=12)\n",
    "    plt.ylabel(f'{min_samples}-th Nearest Neighbor Distance', fontsize=12)\n",
    "    plt.title('K-Distance Graph (Zoomed In)', fontsize=16, weight='bold')\n",
    "    plt.ylim(0, 0.05) # Adjust this Y-limit to find your elbow\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nLook for the 'elbow' in the zoomed graph to determine optimal eps.\")\n",
    "else:\n",
    "    print(\"Cannot determine DBSCAN parameters: Sample coordinate data not created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3625cd8",
   "metadata": {},
   "source": [
    "## 6. Apply DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e1953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will fail if Lat/Lon are missing\n",
    "if 'coords_sample' in locals():\n",
    "    # Set DBSCAN parameters (use the 'eps' value from your graph)\n",
    "    eps = 0.01  # <<<--- ADJUST THIS based on your k-distance graph\n",
    "    min_samples = 4\n",
    "\n",
    "    print(f\"Running DBSCAN with eps={eps} and min_samples={min_samples}...\")\n",
    "    print(f\"Clustering {len(coords_sample):,} incidents...\")\n",
    "\n",
    "    # Apply DBSCAN\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
    "    clusters = dbscan.fit_predict(coords_sample)\n",
    "\n",
    "    # Add cluster labels to dataframe\n",
    "    df_sample['Cluster'] = clusters\n",
    "\n",
    "    # Analyze results\n",
    "    n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "    n_noise = list(clusters).count(-1)\n",
    "    n_clustered = len(clusters) - n_noise\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DBSCAN CLUSTERING RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Number of clusters found: {n_clusters:,}\")\n",
    "    print(f\"Noise points (outliers): {n_noise:,} ({100*n_noise/len(clusters):.2f}%)\")\n",
    "    print(f\"Clustered points: {n_clustered:,} ({100*n_clustered/len(clusters):.2f}%)\")\n",
    "    \n",
    "    if n_clusters > 0:\n",
    "        print(f\"Average cluster size: {n_clustered/n_clusters:.1f} incidents\")\n",
    "        # Display cluster size distribution\n",
    "        cluster_sizes = df_sample[df_sample['Cluster'] != -1].groupby('Cluster').size()\n",
    "        print(f\"\\nCluster size statistics:\")\n",
    "        print(f\"  Min: {cluster_sizes.min():,} incidents\")\n",
    "        print(f\"  Max: {cluster_sizes.max():,} incidents\")\n",
    "        print(f\"  Median: {cluster_sizes.median():.0f} incidents\")\n",
    "        print(f\"  Mean: {cluster_sizes.mean():.1f} incidents\")\n",
    "else:\n",
    "    print(\"Cannot apply DBSCAN: Sample coordinate data not created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c888d85d",
   "metadata": {},
   "source": [
    "## 7. Visualize Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b1998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will fail if Lat/Lon are missing\n",
    "if 'df_sample' in locals() and 'Cluster' in df_sample.columns:\n",
    "    plt.figure(figsize=(16, 12))\n",
    "\n",
    "    # Plot noise points in gray\n",
    "    noise_mask = df_sample['Cluster'] == -1\n",
    "    plt.scatter(\n",
    "        df_sample.loc[noise_mask, 'Longitude'],\n",
    "        df_sample.loc[noise_mask, 'Latitude'],\n",
    "        c='lightgray',\n",
    "        alpha=0.3,\n",
    "        s=5,\n",
    "        label='Noise/Outliers'\n",
    "    )\n",
    "\n",
    "    # Plot clustered points with colors\n",
    "    clustered_mask = df_sample['Cluster'] != -1\n",
    "    if clustered_mask.sum() > 0:\n",
    "        scatter = plt.scatter(\n",
    "            df_sample.loc[clustered_mask, 'Longitude'],\n",
    "            df_sample.loc[clustered_mask, 'Latitude'],\n",
    "            c=df_sample.loc[clustered_mask, 'Cluster'],\n",
    "            cmap='tab20', # A colormap with many distinct colors\n",
    "            alpha=0.6,\n",
    "            s=10,\n",
    "            edgecolors='none'\n",
    "        )\n",
    "        plt.colorbar(scatter, label='Cluster ID')\n",
    "\n",
    "    plt.xlabel('Longitude', fontsize=12)\n",
    "    plt.ylabel('Latitude', fontsize=12)\n",
    "    plt.title(f'San Jose DBSCAN Clustering Results (eps={eps}, min_samples={min_samples})', \n",
    "              fontsize=16, weight='bold')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save figure\n",
    "    output_dir = Path('../reports/figures')\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(output_dir / '07_sj_dbscan_clusters_overview.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot visualize clusters: Clustered dataframe not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7988c328",
   "metadata": {},
   "source": [
    "## 8. Analyze Top Hotspot Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09f2d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will fail if Lat/Lon are missing\n",
    "if 'df_sample' in locals() and n_clusters > 0:\n",
    "    cluster_info = df_sample[df_sample['Cluster'] != -1].groupby('Cluster').agg({\n",
    "        'Latitude': ['mean', 'count'],\n",
    "        'Longitude': 'mean',\n",
    "        'Neighborhood': lambda x: x.mode()[0] if len(x.mode()) > 0 else 'Unknown'\n",
    "    }).round(6)\n",
    "    \n",
    "    cluster_info.columns = ['Center_Lat', 'Size', 'Center_Lon', 'Neighborhood']\n",
    "    cluster_info = cluster_info.sort_values('Size', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TOP 20 SAN JOSE INCIDENT HOTSPOTS (Largest Clusters)\")\n",
    "    print(\"=\"*80)\n",
    "    print(cluster_info.head(20).to_string())\n",
    "    \n",
    "else:\n",
    "    print(\"No clusters found to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06578162",
   "metadata": {},
   "source": [
    "## 9. Summary and Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c7fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will fail if Lat/Lon are missing\n",
    "if 'df_sample' in locals() and n_clusters > 0:\n",
    "    print(\"=\"*80)\n",
    "    print(\"SPATIAL ANALYSIS SUMMARY - SAN JOSE INCIDENTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nðŸ“Š Dataset Information:\")\n",
    "    print(f\"   â€¢ Total incidents in source file: {len(df):,}\")\n",
    "    print(f\"   â€¢ Incidents sampled for analysis: {len(df_sample):,}\")\n",
    "    print(f\"   â€¢ Date range: {df_sample.index.min().date()} to {df_sample.index.max().date()}\")\n",
    "\n",
    "    print(f\"\\nðŸŽ¯ DBSCAN Parameters:\")\n",
    "    print(f\"   â€¢ Epsilon (eps): {eps}\")\n",
    "    print(f\"   â€¢ Minimum samples: {min_samples}\")\n",
    "\n",
    "    print(f\"\\nðŸ—ºï¸ Clustering Results:\")\n",
    "    print(f\"   â€¢ Number of clusters identified: {n_clusters:,}\")\n",
    "    print(f\"   â€¢ Incidents in clusters: {n_clustered:,} ({100*n_clustered/len(clusters):.1f}%)\")\n",
    "    print(f\"   â€¢ Noise/outlier points: {n_noise:,} ({100*n_noise/len(clusters):.1f}%)\")\n",
    "\n",
    "    if n_clusters > 0:\n",
    "        print(f\"\\nðŸ“ˆ Cluster Statistics:\")\n",
    "        print(f\"   â€¢ Average cluster size: {cluster_sizes.mean():.1f} incidents\")\n",
    "        print(f\"   â€¢ Median cluster size: {cluster_sizes.median():.0f} incidents\")\n",
    "        print(f\"   â€¢ Largest cluster: {cluster_sizes.max():,} incidents\")\n",
    "        print(f\"   â€¢ Smallest cluster: {cluster_sizes.min():,} incidents\")\n",
    "        \n",
    "        print(f\"\\nðŸ”¥ Top 5 Hotspots (by Neighborhood):\")\n",
    "        for i, (cluster_id, row) in enumerate(cluster_info.head(5).iterrows(), 1):\n",
    "            print(f\"   {i}. Cluster {cluster_id}: {row['Size']:,} incidents - {row['Neighborhood']}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "else:\n",
    "    print(\"Cannot generate summary: Clustering was not performed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SF Crime Analysis)",
   "language": "python",
   "name": "sjsf-crime-analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

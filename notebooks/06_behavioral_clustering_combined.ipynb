{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "914378b6",
   "metadata": {},
   "source": [
    "# Behavioral Clustering Analysis: San Francisco & San Jose Combined\n",
    "\n",
    "**Project:** A Tale of Two Cities - Comparative Public Safety Analysis\n",
    "\n",
    "**Purpose:** This notebook performs behavioral clustering using K-Means to identify distinct incident patterns across both cities based on temporal features, incident types, and contextual characteristics.\n",
    "\n",
    "**Key Objectives:**\n",
    "- Engineer temporal and behavioral features from incident data\n",
    "- Identify 4-6 distinct behavioral incident profiles using K-Means\n",
    "- Compare incident behavior patterns between San Francisco and San Jose\n",
    "- Validate clustering quality using multiple metrics\n",
    "- Generate actionable insights for public safety resource allocation\n",
    "\n",
    "**Expected Deliverables:**\n",
    "- 4-6 behavioral incident profiles with statistical characteristics\n",
    "- 5+ professional visualizations (temporal patterns, radar charts, heatmaps)\n",
    "- Cross-city behavioral comparison analysis\n",
    "- Validation metrics (silhouette scores, statistical tests)\n",
    "- Key insights on incident behavior types\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology Overview\n",
    "\n",
    "**Algorithm:** K-Means clustering with optimal K selection (4-6 expected)\n",
    "\n",
    "**Feature Categories:**\n",
    "1. **Temporal Features:** Hour, day of week, month, season, weekend/weekday flags\n",
    "2. **Incident Characteristics:** High-level categories, subcategory distributions\n",
    "3. **Contextual Features:** Police districts, geographic patterns\n",
    "4. **Derived Metrics:** Incident frequency patterns, temporal densities\n",
    "\n",
    "**Analysis Approach:**\n",
    "- Feature engineering and scaling (StandardScaler)\n",
    "- Optimal K determination (Elbow Method + Silhouette Analysis)\n",
    "- Cluster profiling and interpretation\n",
    "- Cross-city behavioral pattern comparison\n",
    "- Statistical validation (chi-squared tests, silhouette scores)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f017a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Clustering and ML\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_samples, \\\n",
    "                            silhouette_score, \\\n",
    "                            calinski_harabasz_score, \\\n",
    "                            davies_bouldin_score\n",
    "\n",
    "# Statistical tests\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Date/time handling\n",
    "from datetime import datetime\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 10)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "plt.rcParams[\"font.size\"] = 11\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b737b72",
   "metadata": {},
   "source": [
    "## 2. Load and Combine Data\n",
    "\n",
    "**Objective:** Load cleaned incident data from both cities and combine into a single dataset for comparative behavioral analysis.\n",
    "\n",
    "**Data Sources:**\n",
    "- San Francisco: `../data/processed/sf_incidents_cleaned.csv`\n",
    "- San Jose: `../data/processed/sj_calls_cleaned.csv`\n",
    "\n",
    "**Expected Outcome:** Combined dataset with ~200K+ incidents, city identifier column, and harmonized features for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b7d570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading San Francisco data...\n",
      "    SF loaded: 823,541 incidents\n",
      "    Date range: 2018-01-01 to 2025-11-16\n",
      "\n",
      " Loading San Jose data...\n",
      "    SJ loaded: 1,170,667 incidents\n",
      "    Date range: 2018-01-01 to 2025-11-15\n",
      "\n",
      " Combining datasets...\n",
      "    Combined dataset: 1,994,208 total incidents\n",
      "\n",
      "================================================================================\n",
      "COMBINED DATASET SUMMARY\n",
      "================================================================================\n",
      "\n",
      " Total incidents: 1,994,208\n",
      "    San Francisco: 823,541 (41.3%)\n",
      "    San Jose: 1,170,667 (58.7%)\n",
      "\n",
      " Combined date range: 2018-01-01 to 2025-11-16\n",
      "\n",
      " Available columns: ['Incident_High_Level_Category', 'Resolution', 'Neighborhood', 'Police_District', 'Latitude', 'Longitude', 'Hour', 'Day', 'Month', 'Year', 'Day_of_Week', 'Day_of_Week_Name', 'Month_Name', 'Quarter', 'Is_Weekend', 'City']\n"
     ]
    }
   ],
   "source": [
    "# Load San Francisco data\n",
    "print(\"\\n Loading San Francisco data...\")\n",
    "df_sf = pd.read_csv(\n",
    "    '../data/processed/sf_incidents_cleaned.csv',\n",
    "    index_col='Incident DateTime',\n",
    "    parse_dates=True\n",
    ")\n",
    "print(f\"    SF loaded: {len(df_sf):,} incidents\")\n",
    "print(f\"    Date range: {df_sf.index.min().date()} to {df_sf.index.max().date()}\")\n",
    "\n",
    "# Load San Jose data\n",
    "print(\"\\n Loading San Jose data...\")\n",
    "df_sj = pd.read_csv(\n",
    "    '../data/processed/sj_calls_cleaned.csv',\n",
    "    index_col='Incident DateTime',\n",
    "    parse_dates=True\n",
    ")\n",
    "print(f\"    SJ loaded: {len(df_sj):,} incidents\")\n",
    "print(f\"    Date range: {df_sj.index.min().date()} to {df_sj.index.max().date()}\")\n",
    "\n",
    "# Add city identifier column\n",
    "df_sf['City'] = 'San Francisco'\n",
    "df_sj['City'] = 'San Jose'\n",
    "\n",
    "# Combine datasets\n",
    "print(\"\\n Combining datasets...\")\n",
    "df_combined = pd.concat([df_sf, df_sj], axis=0)\n",
    "print(f\"    Combined dataset: {len(df_combined):,} total incidents\")\n",
    "\n",
    "# Display combined dataset info\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMBINED DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n Total incidents: {len(df_combined):,}\")\n",
    "print(f\"    San Francisco: {len(df_sf):,} ({100*len(df_sf)/len(df_combined):.1f}%)\")\n",
    "print(f\"    San Jose: {len(df_sj):,} ({100*len(df_sj)/len(df_combined):.1f}%)\")\n",
    "print(f\"\\n Combined date range: {df_combined.index.min().date()} to {df_combined.index.max().date()}\")\n",
    "print(f\"\\n Available columns: {df_combined.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb9010e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Data - San Francisco:\n",
      "                     Hour Day_of_Week_Name  Is_Weekend  \\\n",
      "Incident DateTime                                        \n",
      "2025-08-27 00:37:00     0        Wednesday           0   \n",
      "2025-07-17 15:00:00    15         Thursday           0   \n",
      "\n",
      "                    Incident_High_Level_Category  \n",
      "Incident DateTime                                 \n",
      "2025-08-27 00:37:00                      Violent  \n",
      "2025-07-17 15:00:00                        Fraud  \n",
      "\n",
      "Sample Data - San Jose:\n",
      "                     Hour Day_of_Week_Name  Is_Weekend  \\\n",
      "Incident DateTime                                        \n",
      "2018-01-01 00:00:02     0           Monday           0   \n",
      "2018-01-01 00:00:15     0           Monday           0   \n",
      "\n",
      "                    Incident_High_Level_Category  \n",
      "Incident DateTime                                 \n",
      "2018-01-01 00:00:02                        Other  \n",
      "2018-01-01 00:00:15                        Alarm  \n"
     ]
    }
   ],
   "source": [
    "temporal_features = ['Hour', 'Day', 'Month', 'Year', 'Day_of_Week', 'Day_of_Week_Name', \n",
    "                     'Month_Name', 'Quarter', 'Is_Weekend']\n",
    "for feature in temporal_features:\n",
    "    sf_status = True if feature in df_sf.columns else False\n",
    "    sj_status = True if feature in df_sj.columns else False\n",
    "    if not sf_status:\n",
    "        print(f\"SF missing temporal feature: {feature}\")\n",
    "    if not sj_status:\n",
    "        print(f\"SJ missing temporal feature: {feature}\")\n",
    "        \n",
    "\n",
    "categorical_features = ['Incident_High_Level_Category', 'Police_District', 'Neighborhood']\n",
    "for feature in categorical_features:\n",
    "    sf_status = True if feature in df_sf.columns else False\n",
    "    sj_status = True if feature in df_sj.columns else False\n",
    "    if not sf_status:\n",
    "        print(f\"SF missing categorical feature: {feature}\")\n",
    "    if not sj_status:\n",
    "        print(f\"SJ missing categorical feature: {feature}\")\n",
    "\n",
    "print(\"\\nSample Data - San Francisco:\")\n",
    "print(df_sf[['Hour', 'Day_of_Week_Name', 'Is_Weekend', 'Incident_High_Level_Category']].head(2))\n",
    "\n",
    "print(\"\\nSample Data - San Jose:\")\n",
    "print(df_sj[['Hour', 'Day_of_Week_Name', 'Is_Weekend', 'Incident_High_Level_Category']].head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861af38",
   "metadata": {},
   "source": [
    "### Feature Inspection Results\n",
    "\n",
    "- **All temporal features present** in both datasets\n",
    "  - Hour (0-23), Day, Month, Year\n",
    "  - Day_of_Week (0-6), Day_of_Week_Name\n",
    "  - Quarter (1-4), Is_Weekend (0/1)\n",
    "- **All categorical features present** in both datasets\n",
    "  - Incident_High_Level_Category\n",
    "  - Police_District, Neighborhood\n",
    "- **Feature structure is identical** between cities\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71e3a68",
   "metadata": {},
   "source": [
    "## 3. Feature Selection for Behavioral Clustering\n",
    "\n",
    "**Objective:** Select and prepare features that capture incident **behavior patterns** for K-Means clustering.\n",
    "\n",
    "**Key Insight:** \n",
    "We're discovering behavioral profiles (e.g., \"Late-Night Entertainment District Incidents\"), not just temporal trends.\n",
    "\n",
    "**Selected Clustering Features:**\n",
    "\n",
    "1. **Temporal Behavior (4 features):**\n",
    "   - `Hour` - Time-of-day activity pattern (0-23)\n",
    "   - `Day_of_Week` - Weekday pattern (0=Monday, 6=Sunday)\n",
    "   - `Is_Weekend` - Weekend flag (0/1)\n",
    "   - `Quarter` - Seasonal pattern (1-4)\n",
    "\n",
    "2. **Incident Behavior (1 feature):**\n",
    "   - `Incident_High_Level_Category` - Crime type (will be encoded)\n",
    "\n",
    "3. **Geographic Behavior (1 feature):**\n",
    "   - `Police_District` - District-level patterns (will be encoded)\n",
    "\n",
    "4. **City Identifier (1 feature):**\n",
    "   - `City` - For cross-city comparison (SF vs SJ)\n",
    "\n",
    "**Total Features for Clustering:** 7 base features → Will expand after encoding categorical variables\n",
    "\n",
    "**Why These Features?**\n",
    "- Capture **when** incidents happen (temporal)\n",
    "- Capture **what type** (category)\n",
    "- Capture **where generally** (district, not exact coordinates)\n",
    "- Enable **cross-city comparison** (city identifier)\n",
    "\n",
    "**Expected Outcome:** Feature matrix ready for K-Means with scaled numerical features and encoded categorical variables.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8ad9a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Selected Feature Groups:\n",
      "\n",
      "    Temporal Features (4):\n",
      "      • Hour\n",
      "      • Day_of_Week\n",
      "      • Is_Weekend\n",
      "      • Quarter\n",
      "\n",
      "    Categorical Features (2):\n",
      "      • Incident_High_Level_Category\n",
      "      • Police_District\n",
      "\n",
      "    Identifier (1):\n",
      "      • City\n",
      "\n",
      " Clustering dataset created: 1,994,208 incidents\n",
      "   • Features: 7 columns\n",
      "   • SF incidents: 823,541\n",
      "   • SJ incidents: 1,170,667\n",
      "\n",
      " Feature Value Ranges:\n",
      "\n",
      "   Hour: 0-23\n",
      "   Day_of_Week: 0-6\n",
      "   Is_Weekend: [0 1]\n",
      "   Quarter: [3 2 1 4]\n",
      "   Incident Categories: 8 unique\n",
      "   Police Districts: 12 unique\n"
     ]
    }
   ],
   "source": [
    "clustering_features = {\n",
    "    'temporal': ['Hour', 'Day_of_Week', 'Is_Weekend', 'Quarter'],\n",
    "    'categorical': ['Incident_High_Level_Category', 'Police_District'],\n",
    "    'identifier': ['City']\n",
    "}\n",
    "\n",
    "print(\"\\n Selected Feature Groups:\")\n",
    "print(f\"\\n    Temporal Features ({len(clustering_features['temporal'])}):\")\n",
    "for feat in clustering_features['temporal']:\n",
    "    print(f\"      • {feat}\")\n",
    "\n",
    "print(f\"\\n    Categorical Features ({len(clustering_features['categorical'])}):\")\n",
    "for feat in clustering_features['categorical']:\n",
    "    print(f\"      • {feat}\")\n",
    "\n",
    "print(f\"\\n    Identifier ({len(clustering_features['identifier'])}):\")\n",
    "for feat in clustering_features['identifier']:\n",
    "    print(f\"      • {feat}\")\n",
    "\n",
    "# Create working dataset with selected features\n",
    "all_features = (clustering_features['temporal'] + \n",
    "                clustering_features['categorical'] + \n",
    "                clustering_features['identifier'])\n",
    "\n",
    "df_clustering = df_combined[all_features].copy()\n",
    "\n",
    "print(f\"\\n Clustering dataset created: {len(df_clustering):,} incidents\")\n",
    "print(f\"   • Features: {len(all_features)} columns\")\n",
    "print(f\"   • SF incidents: {len(df_clustering[df_clustering['City']=='San Francisco']):,}\")\n",
    "print(f\"   • SJ incidents: {len(df_clustering[df_clustering['City']=='San Jose']):,}\")\n",
    "\n",
    "# Display feature value ranges\n",
    "print(\"\\n Feature Value Ranges:\")\n",
    "print(f\"\\n   Hour: {df_clustering['Hour'].min()}-{df_clustering['Hour'].max()}\")\n",
    "print(f\"   Day_of_Week: {df_clustering['Day_of_Week'].min()}-{df_clustering['Day_of_Week'].max()}\")\n",
    "print(f\"   Is_Weekend: {df_clustering['Is_Weekend'].unique()}\")\n",
    "print(f\"   Quarter: {df_clustering['Quarter'].unique()}\")\n",
    "print(f\"   Incident Categories: {df_clustering['Incident_High_Level_Category'].nunique()} unique\")\n",
    "print(f\"   Police Districts: {df_clustering['Police_District'].nunique()} unique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ef1541b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample of clustering dataset:\n",
      "                     Hour  Day_of_Week  Is_Weekend  Quarter  \\\n",
      "Incident DateTime                                             \n",
      "2025-08-27 00:37:00     0            2           0        3   \n",
      "2025-07-17 15:00:00    15            3           0        3   \n",
      "2025-08-23 21:30:00    21            5           1        3   \n",
      "\n",
      "                    Incident_High_Level_Category Police_District  \\\n",
      "Incident DateTime                                                  \n",
      "2025-08-27 00:37:00                      Violent            Park   \n",
      "2025-07-17 15:00:00                        Fraud            Park   \n",
      "2025-08-23 21:30:00               Theft/Property        Northern   \n",
      "\n",
      "                              City  \n",
      "Incident DateTime                   \n",
      "2025-08-27 00:37:00  San Francisco  \n",
      "2025-07-17 15:00:00  San Francisco  \n",
      "2025-08-23 21:30:00  San Francisco  \n"
     ]
    }
   ],
   "source": [
    "# Display sample\n",
    "print(\"\\n Sample of clustering dataset:\")\n",
    "print(df_clustering.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b744d2",
   "metadata": {},
   "source": [
    "## 4. Feature Encoding and Scaling\n",
    "\n",
    "**Objective:** Transform features into numerical format suitable for K-Means clustering.\n",
    "\n",
    "**Why This Step is Critical:**\n",
    "- K-Means requires all features to be numerical\n",
    "- Features must be on similar scales (Hour: 0-23 vs Is_Weekend: 0-1)\n",
    "- Categorical variables need encoding (e.g., \"Theft\" → numerical representation)\n",
    "\n",
    "**Transformation Steps:**\n",
    "\n",
    "1. **One-Hot Encoding for Categorical Features:**\n",
    "   - `Incident_High_Level_Category` → Multiple binary columns\n",
    "   - `Police_District` → Multiple binary columns\n",
    "   - Prevents ordinal assumptions (no \"Theft > Assault\" relationship)\n",
    "\n",
    "2. **Standardization for Numerical Features:**\n",
    "   - Hour, Day_of_Week, Quarter → Mean=0, StdDev=1\n",
    "   - Ensures equal weight in distance calculations\n",
    "   - Uses StandardScaler from scikit-learn\n",
    "\n",
    "3. **Feature Matrix Creation:**\n",
    "   - Combine encoded categorical + scaled numerical\n",
    "   - Final matrix ready for K-Means algorithm\n",
    "\n",
    "**Expected Outcome:** Numerical feature matrix with ~20-30 dimensions (after encoding) ready for clustering.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "508f3994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/3] One-Hot Encoding Categorical Features...\n",
      "One Hot Encoding Completed.\n",
      "\n",
      "[2/3] Separating City Identifier...\n",
      "   City labels stored separately\n",
      "   Feature matrix shape: (1994208, 22)\n",
      "\n",
      "[3/3] Scaling Features with StandardScaler...\n",
      "   Scaled feature matrix shape: (1994208, 22)\n",
      "   Total features for clustering: 22\n"
     ]
    }
   ],
   "source": [
    "# Step 1: One-Hot Encode Categorical Features\n",
    "print(\"\\n[1/3] One-Hot Encoding Categorical Features...\")\n",
    "\n",
    "categorical_cols = ['Incident_High_Level_Category', 'Police_District']\n",
    "df_encoded = pd.get_dummies(\n",
    "    df_clustering, \n",
    "    columns=categorical_cols,\n",
    "    prefix=categorical_cols,\n",
    "    drop_first=True  # Avoid multicollinearity\n",
    ")\n",
    "\n",
    "print(\"One Hot Encoding Completed.\")\n",
    "\n",
    "# Step 2: Separate target labels (City) from features\n",
    "print(\"\\n[2/3] Separating City Identifier...\")\n",
    "city_labels = df_encoded['City'].copy()\n",
    "df_encoded = df_encoded.drop('City', axis=1)\n",
    "\n",
    "print(f\"   City labels stored separately\")\n",
    "print(f\"   Feature matrix shape: {df_encoded.shape}\")\n",
    "\n",
    "# Step 3: Scale Numerical Features\n",
    "print(\"\\n[3/3] Scaling Features with StandardScaler...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feature_matrix = scaler.fit_transform(df_encoded)\n",
    "\n",
    "print(f\"   Scaled feature matrix shape: {feature_matrix.shape}\")\n",
    "print(f\"   Total features for clustering: {feature_matrix.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b6c2aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Features: ['Hour', 'Day_of_Week', 'Is_Weekend', 'Quarter', 'Incident_High_Level_Category_Disturbance/Suspicious', 'Incident_High_Level_Category_Fraud', 'Incident_High_Level_Category_Non-Criminal/Admin', 'Incident_High_Level_Category_Other', 'Incident_High_Level_Category_Theft/Property', 'Incident_High_Level_Category_Traffic/Vehicle', 'Incident_High_Level_Category_Violent', 'Police_District_Central', 'Police_District_Ingleside', 'Police_District_Mission', 'Police_District_Northern', 'Police_District_Out of SF', 'Police_District_Park', 'Police_District_Richmond', 'Police_District_San Jose', 'Police_District_Southern', 'Police_District_Taraval', 'Police_District_Tenderloin']\n",
      "\n",
      " Feature Categories After Encoding:\n",
      "   Temporal features: 4\n",
      "   Incident category features: 7\n",
      "   Police district features: 11\n",
      "   Total: 22\n",
      "\n",
      "================================================================================\n",
      "FEATURE PREPARATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Ready for K-Means clustering:\n",
      "   Incidents: 1,994,208\n",
      "   Features: 22\n",
      "   Data type: Scaled numerical matrix\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Store feature names for later interpretation\n",
    "feature_names = df_encoded.columns.tolist()\n",
    "print(f\" Features: {feature_names}\")\n",
    "\n",
    "print(\"\\n Feature Categories After Encoding:\")\n",
    "temporal_count = len([f for f in feature_names if f in ['Hour', 'Day_of_Week', 'Is_Weekend', 'Quarter']])\n",
    "category_count = len([f for f in feature_names if 'Incident_High_Level_Category' in f])\n",
    "district_count = len([f for f in feature_names if 'Police_District' in f])\n",
    "\n",
    "print(f\"   Temporal features: {temporal_count}\")\n",
    "print(f\"   Incident category features: {category_count}\")\n",
    "print(f\"   Police district features: {district_count}\")\n",
    "print(f\"   Total: {len(feature_names)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE PREPARATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nReady for K-Means clustering:\")\n",
    "print(f\"   Incidents: {feature_matrix.shape[0]:,}\")\n",
    "print(f\"   Features: {feature_matrix.shape[1]}\")\n",
    "print(f\"   Data type: Scaled numerical matrix\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94ee6f0",
   "metadata": {},
   "source": [
    "### Feature Encoding and Scaling\n",
    "\n",
    "1. **One-hot encoded** categorical variables (Incident_High_Level_Category, Police_District)\n",
    "2. **Separated** City identifier for post-clustering analysis\n",
    "3. **Standardized** all features using StandardScaler (mean=0, std=1)\n",
    "4. **Created** final numerical feature matrix for K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660918b2",
   "metadata": {},
   "source": [
    "## 5. Determine Optimal Number of Clusters (K)\n",
    "\n",
    "**Objective:** Use Elbow Method and Silhouette Analysis to determine the optimal number of behavioral profiles (clusters).\n",
    "\n",
    "**Methods Used:**\n",
    "\n",
    "1. **Elbow Method:**\n",
    "   - Plots inertia (within-cluster sum of squares) vs. K\n",
    "   - Look for \"elbow\" where inertia decrease slows down\n",
    "   - Indicates diminishing returns from adding more clusters\n",
    "\n",
    "2. **Silhouette Analysis:**\n",
    "   - Measures how similar incidents are to their own cluster vs. other clusters\n",
    "   - Score range: -1 (poor) to +1 (excellent)\n",
    "   - Higher average silhouette score indicates better-defined clusters\n",
    "\n",
    "3. **Additional Validation Metrics:**\n",
    "   - Calinski-Harabasz Score: Ratio of between-cluster to within-cluster variance (higher is better)\n",
    "   - Davies-Bouldin Index: Average similarity between clusters (lower is better)\n",
    "\n",
    "**Testing K Range:** 2 to 10 clusters\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318afe48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing K values from 2 to 10...\n",
      "This may take several minutes for large datasets...\n",
      "\n",
      "Testing K=2... "
     ]
    }
   ],
   "source": [
    "k_range = range(2, 11)\n",
    "\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "calinski_scores = []\n",
    "davies_bouldin_scores = []\n",
    "\n",
    "print(f\"\\nTesting K values from {min(k_range)} to {max(k_range)}...\")\n",
    "print(\"This may take several minutes for large datasets...\\n\")\n",
    "\n",
    "# Test each K value\n",
    "for k in k_range:\n",
    "    print(f\"Testing K={k}...\", end=\" \")\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)\n",
    "    cluster_labels = kmeans.fit_predict(feature_matrix)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    inertia = kmeans.inertia_\n",
    "    sil_score = silhouette_score(feature_matrix, cluster_labels)\n",
    "    cal_score = calinski_harabasz_score(feature_matrix, cluster_labels)\n",
    "    db_score = davies_bouldin_score(feature_matrix, cluster_labels)\n",
    "    \n",
    "    # Store results\n",
    "    inertias.append(inertia)\n",
    "    silhouette_scores.append(sil_score)\n",
    "    calinski_scores.append(cal_score)\n",
    "    davies_bouldin_scores.append(db_score)\n",
    "    \n",
    "    print(f\"Silhouette: {sil_score:.4f}, Inertia: {inertia:,.0f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
